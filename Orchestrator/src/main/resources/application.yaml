server:
  port: 9917

spring:
  integration:
    jdbc:
      initialize-schema: always
  application:
    name: ascend-ai-orchestrator
  datasource:
    url: jdbc:postgresql://localhost:5432/ascend_ai
    username: postgres
    password: local
    driver-class-name: org.postgresql.Driver
  ai:
    mcp:
      client:
        enabled: true
        name: AscendAI-Orchestrator
        version: 0.0.1
        request-timeout: 30s

        type: SYNC

        toolcallback:
          enabled: true

        sse:
          connections:
            openmemory:
              url: http://localhost:8765
              sse-endpoint: /mcp/ascendai/sse/user1
        streamable-http:
          connections:
            weather-server:
              url: http://localhost:9998

#        stdio:
#          servers-configuration: classpath:mcp-servers-config.json

    openai:
      api-key: "not-needed" # Not required for local LM Studio
      chat:
        options:
          model: "meta-llama-3.1-8b-instruct" # The exact model ID to be used
          temperature: 0.7
          max-tokens: -1
        base-url: "http://127.0.0.1:1234"

app:
  ingestion:
    chunk-size: 500
  unstructured:
    base-url: http://localhost:9080

s3:
  bucket: knowledge-base
  endpoint: http://localhost:9070
  access-key: admin
  secret-key: password

logging:
  level:
    com.lukk.ai.orchestrator: DEBUG
    org.springframework.ai: INFO
    org.springframework.ai.mcp: INFO
    org.springframework.ai.tool: INFO
    org.springframework.ai.mcp.client.autoconfigure: INFO
    org.springframework.core.io: INFO
    io.modelcontextprotocol.client: INFO
    io.modelcontextprotocol.spec: INFO

    org.springframework.web.client: INFO
    org.apache.http: INFO
